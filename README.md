# ðŸ¦ Risk Banking - Plateforme d'Analyse de Risque CrÃ©dit

---

Ce projet est une application web complÃ¨te conÃ§ue pour aider les conseillers bancaires et les analystes de risque Ã  Ã©valuer et comprendre le risque de dÃ©faut de paiement des clients. L'application se compose d'une interface utilisateur interactive construite avec Streamlit, qui communique avec une API Flask servant de middleware pour orchestrer des calculs complexes sur un cluster Databricks.

## ðŸŒŸ AperÃ§u de l'application
L'application offre deux fonctionnalitÃ©s principales :

**PrÃ©diction de DÃ©faut Client** : Permet d'obtenir une Ã©valuation de risque instantanÃ©e pour un client spÃ©cifique, incluant un score de probabilitÃ©, une recommandation claire (Acceptation, Refus, etc.), et les facteurs influenÃ§ant la dÃ©cision.

**Analyse de DonnÃ©es CrÃ©dit** : Offre un tableau de bord interactif pour explorer les tendances du portefeuille de crÃ©dits Ã  travers 8 analyses thÃ©matiques (par Ã¢ge, par revenu, etc.) avec des filtres dynamiques.

## ðŸ—ï¸ Architecture Technique
Le projet est basÃ© sur une architecture microservices moderne, conÃ§ue pour sÃ©parer les responsabilitÃ©s et permettre une scalabilitÃ© efficace.

```mermaid
graph TD
    A[Utilisateur (Conseiller/Analyste)] -- Interagit avec --> B(Frontend: Streamlit);
    B -- RequÃªtes HTTP --> C{API Backend: Flask};
    C -- Lance des jobs --> D(Calculs Big Data: Databricks);
    C -- RÃ©cupÃ¨re les logs --> E(Monitoring: Azure App Insights);
    C -- RÃ©cupÃ¨re les donnÃ©es personnelles --> F(Base de donnÃ©es: MongoDB);
    D -- Lit les donnÃ©es de crÃ©dit --> G[Stockage: DBFS];
```

**Frontend** (client_prediction.py, data_analysis.py) : Construit avec **Streamlit** pour une interface utilisateur rÃ©active et facile Ã  dÃ©velopper.

**Backend / Middleware** (app.py) : Une **API Flask** qui sert de pont. Elle reÃ§oit les requÃªtes de l'interface, lance les jobs sur Databricks, et formate les rÃ©ponses.

**Moteur de Calcul** : Un cluster **Azure Databricks** exÃ©cute des scripts **PySpark** pour les analyses de donnÃ©es et les prÃ©dictions de modÃ¨les de Machine Learning.

**Stockage de DonnÃ©es** : Les donnÃ©es de crÃ©dit sont stockÃ©es sur **Databricks File System (DBFS)** et les donnÃ©es personnelles des clients sur **MongoDB Atlas**.

**Monitoring & Logging** : Les feedbacks sont centralisÃ©s dans **Azure Application Insights** pour un suivi en temps rÃ©el.

## âœ¨ FonctionnalitÃ©s ClÃ©s

### Page de PrÃ©diction de DÃ©faut Client
- [x] Recherche de client par ID.

- [x] Affichage des informations personnelles du client (nom, photo) depuis MongoDB.

- [x] Jauge de risque visuelle (de 0 Ã  100%).

- [x] Recommandation claire et plan d'action dÃ©taillÃ© pour le conseiller.

- [x] SystÃ¨me de feedback pour signaler les prÃ©dictions incorrectes Ã  App Insights.

### Page d'Analyse de DonnÃ©es CrÃ©dit
- [x] 8 analyses thÃ©matiques interactives (Ã‚ge, Revenu, AnciennetÃ©, etc.).

- [x] Filtres dynamiques pour affiner les analyses (montant, revenu, statut familial...).

- [x] Visualisations de donnÃ©es interactives gÃ©nÃ©rÃ©es avec Plotly.

- [x] Indicateurs clÃ©s (KPIs) rÃ©sumant les principaux insights.

- [x] Recommandations opÃ©rationnelles et stratÃ©giques basÃ©es sur les tendances observÃ©es.

## ðŸ› ï¸ Stack Technique
**Frontend**: Streamlit

**Backend** : Flask

**Calcul Big Data** : Azure Databricks, Apache Spark (PySpark)

**Base de DonnÃ©es** : MongoDB

**Visualisation** : Plotly

**Monitoring** : Azure Application Insights

**Langage** : Python 3.9+

## ðŸš€ Installation et Lancement

Suivez ces Ã©tapes pour lancer le projet en local.

### PrÃ©requis
- Python 3.9 ou supÃ©rieur
- Un compte Azure avec un cluster Databricks configurÃ©.
- Un compte MongoDB Atlas avec les donnÃ©es clients.
- Les identifiants pour Databricks, MongoDB et App Insights.

### 1. Cloner le DÃ©pÃ´t
```bash
git clone [https://github.com/EE-LL/MiseEnOeuvreModelBigDataCloud.git](https://github.com/EE-LL/MiseEnOeuvreModelBigDataCloud.git)
```

### 2. CrÃ©er un Environnement Virtuel
```bash
python -m venv venv
source venv/bin/activate  # Sur Windows: venv\Scripts\activate
```

### 3. Installer les DÃ©pendances
```bash
pip install -r requirements.txt
```

### 4. Configurer les Variables d'Environnement
CrÃ©ez un fichier `.env` Ã  la racine du projet et remplissez-le avec vos informations :

```env
# URL de l'API Flask (pour la communication Streamlit -> Flask)
FLASK_API_URL=[http://127.0.0.1:5001](http://127.0.0.1:5001)

# Identifiants Databricks
DATABRICKS_HOST=[https://adb-xxxxxxxxxxxxxxxx.xx.azuredatabricks.net](https://adb-xxxxxxxxxxxxxxxx.xx.azuredatabricks.net)
DATABRICKS_TOKEN=dapixxxxxxxxxxxxxxxxxxxxxxxx
CLUSTER_ID=xxxx-xxxxxx-xxxxxxxx

# Identifiants MongoDB
MONGODB_URI="mongodb+srv://user:password@cluster..."

# Identifiants Azure Application Insights
CONNECTION_STRING="InstrumentationKey=..."
```

### 5. Lancer l'API Flask
Ouvrez un premier terminal et lancez le serveur backend :

```bash
python app.py
```
Le serveur devrait dÃ©marrer sur `http://127.0.0.1:5001`.

### 6. Lancer l'Application Streamlit
Ouvrez un second terminal et lancez l'interface utilisateur. Vous pouvez choisir quelle page lancer :

**Pour la page de prÃ©diction :**
```bash
streamlit run client_prediction.py
```

**Pour la page d'analyse de donnÃ©es :**
```bash
streamlit run data_analysis.py
```
L'application s'ouvrira automatiquement dans votre navigateur Ã  l'adresse `http://localhost:8501`.

## ðŸ“– Structure du Projet
```
.
â”œâ”€â”€ Home.py                 # Page d'accueil de l'application Streamlit
â”œâ”€â”€ app.py                  # API Backend (Flask)
â”œâ”€â”€ pages/
â”‚   â”œâ”€â”€ 2_PrÃ©diction_Client.py    # Page Streamlit pour la prÃ©diction
â”‚   â””â”€â”€ 3_Analyse_de_DonnÃ©es.py # Page Streamlit pour l'analyse
â”œâ”€â”€ assets/                 # Images
â”‚   â””â”€â”€ logo.png           
â”‚   â””â”€â”€ risk_banking.png         
â”œâ”€â”€ requirements.txt        # DÃ©pendances Python
â”œâ”€â”€ .env                    # Fichier pour les variables d'environnement
â””â”€â”€ README.md               # Ce fichier
